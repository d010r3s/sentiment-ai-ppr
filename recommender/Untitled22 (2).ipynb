{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhFUXEmrYX7n",
        "outputId": "92cdb4b3-9591-4f34-a7e4-828b8e1f1a41"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade \"transformers>=4.38\" accelerate bitsandbytes sentencepiece --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "import torch, pandas as pd, pathlib, datetime, textwrap\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "MODEL_ID = \"IlyaGusev/saiga_mistral_7b_lora\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_ID,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True\n",
        "        ).eval()\n",
        "\n",
        "streamer = TextStreamer(tok)\n"
      ],
      "metadata": {
        "id": "SByjqrQKYeOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, pathlib, chardet, csv, io\n",
        "\n",
        "path = pathlib.Path(\"/content/all_reviews.csv\")\n",
        "\n",
        "encoding = \"windows-1251\"\n",
        "\n",
        "sample = path.read_bytes()[:32_000].decode(encoding, errors=\"replace\")\n",
        "sniff = csv.Sniffer().sniff(sample, delimiters=\",;\\t|\")\n",
        "dialect_delim = sniff.delimiter\n",
        "print(\"Detected delimiter:\", repr(dialect_delim))\n",
        "\n",
        "df = pd.read_csv(\n",
        "        path,\n",
        "        encoding=encoding,\n",
        "        sep=dialect_delim,\n",
        "        engine=\"python\",\n",
        "        quoting=csv.QUOTE_NONE,\n",
        "        on_bad_lines=\"skip\",\n",
        "    )\n",
        "\n",
        "print(\"✅ загружено строк:\", len(df))\n",
        "print(\"Колонки:\", list(df.columns)[:10])\n"
      ],
      "metadata": {
        "id": "d6dQYRa3Yw8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neg = df.loc[df[\"sentiment\"].isin([\"negative\", \"neutral\"]), \"text\"] \\\n",
        "            .astype(str) \\\n",
        "            .tolist()\n",
        "\n",
        "print(\"Негативных отзывов:\", len(neg))\n"
      ],
      "metadata": {
        "id": "rBgdZ2MDcN3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COMPANY_CTX = textwrap.dedent(\"\"\"\n",
        "Компания: «Передовые Платёжные Решения» (ППР)\n",
        "Тип бизнеса: финтех-оператор B2B\n",
        "Клиенты: корпоративные автопарки и командировочные службы (≈80k компаний)\n",
        "Услуги: оплата топлива, штрафы, парковки, платные дороги, отчётность по расходам\n",
        "Цель: снижать негатив клиентов, улучшать поддержку, ускорять возвраты, держать тарифы конкурентными\n",
        "\"\"\").strip()\n",
        "\n",
        "FEWSHOT = textwrap.dedent(\"\"\"\n",
        "### Отзывы:\n",
        "- Слишком долго оформляют возврат.\n",
        "- Операторы не помогают решить вопрос.\n",
        "\n",
        "### Рекомендации:\n",
        "1. Сократить срок возврата средств до 5 дней.\n",
        "2. Ввести KPI операторов: не менее 80 % решённых обращений.\n",
        "\"\"\").strip()\n",
        "\n",
        "FORBID = (\"Не предлагай уходить к конкурентам. \"\n",
        "          \"Не предлагай закрыть счёт или заменить компанию.\")\n",
        "\n",
        "def prompt(batch):\n",
        "    system = (\n",
        "        \"Ты — эксперт по клиентскому опыту. Я — менеджер компании ППР.\"\n",
        "        \"Дай мне до 10 конкретных рекомендаций, как снизить негатив. \"\n",
        "        \"Не предлагай уходить к конкурентам или искать альтернативу.\"\n",
        "        f\"{FORBID} Формат ответа: нумерованный список 4–6 пунктов.\"\n",
        "        \"\\n\\nКонтекст компании и клиентов:\\n\" + COMPANY_CTX\n",
        "    )\n",
        "    user = \"\\n\".join(f\"- {t.strip()}\" for t in batch)\n",
        "    return f\"<s>[INST] <<SYS>>\\n{system}\\n<</SYS>>\\n{FEWSHOT}\\n\\n### Отзывы:\\n{user}\\n\\n### Рекомендации: [/INST]\"\n"
      ],
      "metadata": {
        "id": "3m9VBuFwG9MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import textwrap, torch\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "def batches(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i : i + n]\n",
        "\n",
        "# def strip_banned(text: str) -> str:\n",
        "#     keep = [ln for ln in text.splitlines()\n",
        "#             if not any(bad in ln.lower() for bad in BAD_PHRASES)]\n",
        "#     return \"\\n\".join(keep).strip() or text.strip()\n",
        "\n",
        "def generate_recommendations(reviews, batch=BATCH_SIZE):\n",
        "    output = []\n",
        "    for pack in tqdm(list(batches(reviews, batch)), desc=\"☁️ генерация\", unit=\"batch\"):\n",
        "        inp = tok(prompt(pack), return_tensors=\"pt\").to(model.device)\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(\n",
        "                **inp,\n",
        "                do_sample=True,\n",
        "                temperature=0.8,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.15,\n",
        "                no_repeat_ngram_size=4,\n",
        "                max_new_tokens=256,\n",
        "            )\n",
        "        text = tok.decode(gen[0, inp.input_ids.shape[1]:],\n",
        "                          skip_special_tokens=True).strip()\n",
        "        # output.append(strip_banned(text))\n",
        "        output.append(text)\n",
        "    return output\n",
        "\n",
        "reviews_src = neg if \"neg_clean\" in globals() else neg\n",
        "advices = generate_recommendations(reviews_src)\n",
        "\n",
        "print(\"\\nПример рекомендаций:\\n\")\n",
        "print(advices[0])\n"
      ],
      "metadata": {
        "id": "mDiDgLMAZHDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}